{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8356a4ca-6d2a-4925-a02f-b8563fb26542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MeltModelUAF.ipynb, Kennicott Glacier melt calculator incorporating UAF/Chinese datapoints   \n",
    "### Mike Loso, Updated 8/16/22\n",
    "\n",
    "## Prep temp data\n",
    "This code loads up histograms of the Kennicott Glacier (based off of the 2012 IFSAR and a 2012/2013 outline). There are three histograms: one full histogram, one of debris-covered ice, and one of clean ice. This code uses the latter two to calculate melt in each 10m bin. \n",
    "\n",
    "The temperature rule is: \n",
    "\n",
    "#### ON DEBRIS\n",
    "{temp at elev *x*} = ( {temp at Gates AWS} * 0.571476 + 1.775760 ) - ( 4.86 * ( *x* - 1070) * 0.001 )\n",
    "\n",
    "#### OFF DEBRIS\n",
    "{temp at elev *x*} = ( {temp at Gates AWS} * 0.571476 + 1.775760 ) - ( 3.72 * ( *x* - 1070) * 0.001 )\n",
    "\n",
    "The histograms count numbers of 5m x 5m pixels in each of 456 bins that separate pixels into 10m elevation bands. So area in each elevation band is \"heights\" * 25 m. Histogram columns are {'binedgeR', 'binedgeL', 'bincenters', and 'heights'}.\n",
    "\n",
    "Temperature data come from Gates AWS without any modifications.\n",
    "\n",
    "## Load ablation stake data and calculate melt models\n",
    "Then this code loads up ablation data from NPS stakes and UAF stakes. See notes below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68fff6-1a1f-4736-80fa-aaff5f4e8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT PACKAGES\n",
    "\n",
    "# standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os, csv\n",
    "\n",
    "# specialized\n",
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "# statsmodels is a nice tool for regressions\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy.ma as ma\n",
    "\n",
    "# set default figure size\n",
    "figsize=(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04db02f-cd85-4763-bcf7-fd6cd0b3c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD GATES AWS (NPS LONG TERM) RECORD AND RESAMPLE/CLEAN UP\n",
    "\n",
    "# First load Gates long-term AWS record\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data/Raw_Climate_Records/current 2021/GATES hrly all vars 2016_01_01 to 2021_11_10/from telemetry/'\n",
    "# Load an excel workbook that is in the local directory\n",
    "csv=os.path.join(data_dir,'GGLA2.2021-11-10.csv')\n",
    "# Load the first worksheet as 'wb', treat -9999 as nan|, make first column (dates) the index\n",
    "nps=pd.read_csv(csv,skiprows=[11],na_values=[-9999],header=10,parse_dates=['Date_Time'],usecols=['Date_Time','air_temp_set_1'])\n",
    "# convert zulu time to AK Time\n",
    "nps.loc[:,'AKdate'] = nps['Date_Time'].dt.tz_convert('US/Alaska')\n",
    "# shift timestamp 33 minutes to read on the hour\n",
    "nps.AKdate = nps.AKdate - pd.Timedelta(33,'minutes') # on the hour\n",
    "# keep time as it is but remove time zone info (stays local AK)\n",
    "nps.loc[:,'AKdate'] = nps.AKdate.dt.tz_localize(tz=None)\n",
    "nps=nps.set_index(['AKdate'])\n",
    "# clean up\n",
    "del nps['Date_Time']\n",
    "nps.rename(columns={'air_temp_set_1':'GatesNPS'},inplace=True)\n",
    "\n",
    "# print(nps.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b2290-19de-4dfb-9d98-fd2f8efe8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BINNED ELEVATION DATA PICKLED FROM KENNRIVQ.IPYNB\n",
    "\n",
    "# load pickled elevation data in bins (clean and debris)\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# load data as two pandas dataframes: debris and pickle\n",
    "debris = pd.read_pickle(os.path.join(data_dir, 'debrisbins.pickle'))\n",
    "clean = pd.read_pickle(os.path.join(data_dir, 'cleanbins.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3de9f4-0484-41c7-bdf1-58ae16d15408",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE 2-d ARRAY OF TEMPS, WITH DATETIME (in rows) and ELEVATIONBIN (in columns)\n",
    "## Ultimately 2 arrays: one for clean and one for debris.\n",
    "\n",
    "# There should be 50,567 rows (datetimes) IF we do hourly. (dates and temps available in nps.AKdate and nps.GatesNPS)\n",
    "# There should be 456 columns (bincenters). (bincenters available in clean.bincenters or debris.bincenters)\n",
    "\n",
    "# pre-create empty arrays (one for debris one for clean) to populate\n",
    "number_of_rows = 50567\n",
    "number_of_columns = 456\n",
    "cleantemps=np.zeros((number_of_rows,number_of_columns))\n",
    "# and for dirty ice\n",
    "debristemps=np.zeros((number_of_rows,number_of_columns))\n",
    "\n",
    "# change bincenters to np.array 'centers' and 'datesf' for numpy work\n",
    "centers=debris['bincenters'].to_numpy()\n",
    "# change Gates temp record to np.array 'gatestemps'. single brackets make it 1d array, not a 2d array with 50567 rows and 1 column\n",
    "gatestemps=nps['GatesNPS'].to_numpy()\n",
    "# change Gates index to numpy array of datetimes (note these won't work for plotting surface below, so will change back to float there)\n",
    "dates=nps.index.to_numpy(dtype='datetime64[ns]')\n",
    "\n",
    "\n",
    "#### EDIT THIS TO USE ONLY THE CLEAN ICE LAPSE RATE ABOVE GATES 3600 AT 1070 M ####\n",
    "### BELOW GATES3600: CLEAN ICE = 3.72, DIRTY ICE = 4.86\n",
    "### ABOVE GATES3600: ALL ICE = 3.72\n",
    "\n",
    "\n",
    "# create loop to populate arrays (might be a better way to do this). Iterate over columns (bincenters)\n",
    "# gatestemp is a column array of temperatures over record\n",
    "# cleantemps[:,col] or debristemps[:,i] is the index for the column with the particular bincenter elevation we're working with\n",
    "# starting column index\n",
    "col=0\n",
    "for i in centers:\n",
    "    while i<1070:\n",
    "        # calculate air temp on dirty ice below Gates AWS for each timestep as function of Gates temp and elevation\n",
    "        debristemps[:,col]=((gatestemps)*0.571476 + 1.775760)-(4.86*(i-1070)*.001);\n",
    "        # calculate air temp for each clean ice bin as function of Gates temp\n",
    "        cleantemps[:,col]=((gatestemps)*0.571476+1.775760)-(3.72*(i-1070)*.001);\n",
    "    else:\n",
    "        # calculate air temp on dirty ice above Gates AWS for each timestep as function of Gates temp and elevation (use clean ice lapse rate)\n",
    "        debristemps[:,col]=((gatestemps)*0.571476 + 1.775760)-(3.72*(i-1070)*.001);\n",
    "        # calculate air temp for each clean ice bin as function of Gates temp\n",
    "        cleantemps[:,col]=((gatestemps)*0.571476+1.775760)-(3.72*(i-1070)*.001);\n",
    "    col=col+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d1d3c-877a-4b20-9bf9-ff4ffe7560a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=clean.bincenters\n",
    "print(temp>1245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc683c-1e03-4b81-991c-bfd0c471cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT 2-d ARRAYS OF TEMP VS DATE AND ELEVATION\n",
    "              \n",
    "\n",
    "# Turn dates (dtype=datetime64) into float for purposes of plotting. Call it datesf\n",
    "datesf=nps.index.to_numpy(dtype='float')\n",
    "\n",
    "# For some reason plot_surface requires the x and y axes to be 2d, so expand each to be same shape as data, using meshgrid\n",
    "X, Y = np.meshgrid(datesf,centers)  # `plot_surface` expects `x` and `y` data to be 2D\n",
    "\n",
    "# cleantemps and debristemps are each 50567x456, but have a small fraction (<1%) nans, 7752 to be exact. Turn into zeros.\n",
    "cleantemps_mask=np.isfinite(cleantemps)\n",
    "debristemps_mask=np.isfinite(debristemps)\n",
    "cleanmasked=np.ma.array(cleantemps,mask=np.column_stack((cleantemps_mask)))\n",
    "debrismasked=np.ma.array(debristemps,mask=np.column_stack((debristemps_mask)))\n",
    "\n",
    "## FIGURE 1\n",
    "fig1=plt.subplots(figsize=(10,6))\n",
    "# This next line is key for creating 3d surface plot\n",
    "ax=plt.axes(projection='3d')\n",
    "\n",
    "# Don't know what the '.T' suffix is needed for, but seems to work\n",
    "ax.scatter(X.T,Y.T,debrismasked,s=2,depthshade=True)\n",
    "plt.xlabel('Time from 2016 to present')\n",
    "plt.ylabel('Elev (m)')\n",
    "plt.title('Clean ice temperatures (degC)')\n",
    "\n",
    "## FIGURE 2\n",
    "fig2=plt.subplots(figsize=(10,6))\n",
    "# This next line is key for creating 3d surface plot\n",
    "ax=plt.axes(projection='3d')\n",
    "\n",
    "# Don't know what the '.T' suffix is needed for, but seems to work\n",
    "ax.scatter(X.T,Y.T,debrismasked,s=2,depthshade=True)\n",
    "plt.xlabel('Time from 2016 to present')\n",
    "plt.ylabel('Elev (m)')\n",
    "plt.title('Debris-covered ice temperatures (degC)')\n",
    "                  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100d638-2c66-47f0-b550-0e9c3e4763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT A TIMESLICE OF TEMPERATURES AT ALL ELEVATIONS FOR DEBRIS AND NON-DEBRIS\n",
    "\n",
    "print(cleantemps.shape)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,5))\n",
    "plt.plot(centers,cleantemps[8900,:])\n",
    "plt.plot(centers,debristemps[8900,:])\n",
    "plt.legend({'Temp on clean ice (degC)','Temp on debris-covered ice (degC)'})\n",
    "## NOTE there is some debris-covered ice up to around 1500 m--this relationship would appear to be bogus (colder temps on debris) above about 1050 m. Fix, though a small problem.\n",
    "plt.show()\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c45eb7-97d5-453e-ad55-5852147d8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT A SLICE OF TEMPERATURES AT ALL TIMES FOR ONE TIME FOR DEBRIS AND NON-DEBRIS\n",
    "\n",
    "# set plotting elevation by choosing cell from centers (center[30] is 705 m)\n",
    "elev=30\n",
    "fig,ax=plt.subplots(figsize=(10,5))\n",
    "plt.plot(dates,cleantemps[:,30],linewidth=.1)\n",
    "plt.plot(dates,debristemps[:,30],'r',linewidth=.1)\n",
    "plt.legend({'Temp on clean ice (degC)','Temp on debris-covered ice (degC)'})\n",
    "plt.title(f'Time series of temperatures on clean or dirty ice at {centers[elev]} elevation m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a70e9e-cc61-4bd5-a11c-12819fbfb435",
   "metadata": {},
   "source": [
    "### Next step, import ablation records from NPS and UAF stakes\n",
    "Next I import a record of measured ablation totals from Kennicott Glacier (KennSummerAblation_NPS+UAF.xlsx). The NPS values were exported from the SQL Glaciers database using a saved query and the UAF values are from Eric Petersen via email 2022_03_17, but then later updated 2022_06_09, and again 2022_08_16. Edit that last notebook if you want to change anything. It is exported with the columns 'Site', 'Date_Time', 'Latitude', 'Longitude', 'HAMSL_m', 'Melt_Season_SWE_Change_m', 'LastVisitDate', 'DaysSinceLastVisit'. I manually added 'IceSnow', which describes the surface that was melting at the beginning/end of the melt season, respectively, 'Summ_Accum_m', which could have been part of the query but I didn't think of it, Debris_Thickness_cm, which only applies if it is a debris-covered site, and Institution (NPS or UAF). Note that melt season change ignores summer accum (reflects the surface under the summer accum), and the summ_accum--when it shows up, stops the melt season (if it persists).\n",
    "\n",
    "Also, there is one duplicate record that comes out of the SQL server export as of 12/4/2021: two versions of Kenn6100 in 2019. The one with a melt season change of -2.805 is preferred, so I manually removed a record with change of -2.746. I also separated out 3 records from Gates 7200 that have such small melt responses that I think they skew the DDF calculation, likely because they received significant mid-summer precip. I called them \"snow/snow/suspect\" in the IceSnow category. This could be fixed in db query somehow in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a5fa0-dd92-4fc3-b85a-cc6197545fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DATA\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# Load an excel workbook that is in the local directory\n",
    "xlsx=pd.ExcelFile(os.path.join(data_dir,'KennSummerAblation_NPS+UAF 2022_08_16.xlsx'))\n",
    "# Load the first worksheet, treat -9999 as nan, make second column index (DateTime) \n",
    "wb=pd.read_excel(xlsx,sheet_name='Sheet1',na_values=[-9999],index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60084c0-5ba7-4f2c-8b7d-4b3015282f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VIEW RAW DATA\n",
    "\n",
    "# # Print summary of worksheet ('wb')\n",
    "# print(wb.head())\n",
    "# # Print datatypes (dtypes) of DataFrame (wb)\n",
    "# print(wb.dtypes)\n",
    "\n",
    "# teach it to ignore sites with Debris (~ makes it the opposite of contains)\n",
    "wbclean=wb[(wb['IceSnow'].str.contains('ice/ice|snow/snow', na=False))]\n",
    "\n",
    "# Plot with each site its own color\n",
    "fig, ax=plt.subplots(figsize=(10,10))\n",
    "\n",
    "# group entries by site, then for each group (there are 36 sites including UAF non-debris data)\n",
    "grouped = wbclean.groupby('Site')\n",
    "\n",
    "# create array of colors of length (grouped)\n",
    "jet=plt.get_cmap('jet')\n",
    "colors = iter(jet(np.linspace(0,1,len(grouped))))\n",
    "\n",
    "# loop through each group\n",
    "# key is the ordered list of sites (the grouping variable)\n",
    "# group is the ordered list of rows in each group\n",
    "# use_index forces 'plot' to use the index (Date_Time) as the x axis\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='line', use_index=True, y='Melt_Season_SWE_Change_m', label=key, color=next(colors), marker='o')\n",
    "# increment the colorkey\n",
    "    #colorkey=colorkey+1\n",
    "    ax.legend(loc='lower left',fontsize=8)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Melt season SWE change (m SWE)')\n",
    "    plt.title('Raw melt data for variable time periods')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565aac0-a400-4bf2-87ca-da13aa768a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACT TEMPERATURE HISTORY THAT MATCHES EACH STAKE RECORD SO I CAN COMPUTE DD FOR EACH OBSERVATION\n",
    "\n",
    "# Note that all stakes in wbclean are on clean ice so we'll use the 'cleantemps' record\n",
    "\n",
    "# Create new column and find index of correct binned elevation. wb.centersloc\n",
    "wbclean=wbclean.assign(centersloc='')\n",
    "# make into numpy\n",
    "stakecenters=wbclean['HAMSL_m'].to_numpy()\n",
    "# this finds the indices of values in centers that are just ABOVE values of stake elevations\n",
    "q=centers.searchsorted(stakecenters)\n",
    "# this finds out how far apart the stakes and the bincenters are. \n",
    "diff=centers[q]-stakecenters\n",
    "# where it is greater than 5, we should decrease the index by 1, placing them in the right bin\n",
    "q[diff>5]=q[diff>5]-1\n",
    "# assign to table\n",
    "wbclean.centersloc=q\n",
    "\n",
    "# Create new row and find index for starting date indices in wb\n",
    "wbclean=wbclean.assign(datestartloc='')\n",
    "# Create new row and find index for ending date indices in wb\n",
    "wbclean=wbclean.assign(dateendloc='')\n",
    "# make into numpy\n",
    "art=wbclean['LastVisitDate'].to_numpy()\n",
    "nd=wbclean.index.to_numpy()\n",
    "# artindex+1 are the indices of the values in dates that are just below (AFTER) the first dates of stake measurements\n",
    "artindex=dates.searchsorted(art)+1\n",
    "# ndindex are the indices of the values in dates are are just above (before) the final dates of stake measurements\n",
    "ndindex=dates.searchsorted(nd)\n",
    "# assign to table\n",
    "wbclean.datestartloc=artindex\n",
    "wbclean.dateendloc=ndindex\n",
    "\n",
    "# Create new row for DD in wb\n",
    "wbclean=wbclean.assign(DD='')\n",
    "# loop through to get DD\n",
    "for i in range(0,len(q)):\n",
    "    # create temp vector of appropriate temps\n",
    "    hourlytemps=cleantemps[[range(wbclean.datestartloc[i],wbclean.dateendloc[i])],[wbclean.centersloc[i]]]\n",
    "    #### ???????????????????????????????????????????????????????????????????????????????????????? row below\n",
    "    # there are a TOTAL of 17 hours in the entire record used here that are NaNs. Never more than one in a single interval\n",
    "    # So just turn all NaNs into zeros. Very trivial effect on record. \n",
    "    np.nan_to_num(hourlytemps,copy=False)\n",
    "    # find and turn all zero values into 0\n",
    "    hourlytemps[hourlytemps<0]=0\n",
    "    # calculate degree days\n",
    "    dd=np.sum(hourlytemps*(1/24))\n",
    "    # assign dd to table\n",
    "    wbclean.DD[i]=dd\n",
    "    \n",
    "# This code generates a pandas warning I should fix. But it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f7108-49bd-46e2-8860-fbd547261cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE DD TO MELT\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(10,5))\n",
    "\n",
    "# group entries by ice or snow\n",
    "groups = wbclean.groupby('IceSnow')\n",
    "\n",
    "# loop through each group\n",
    "# name is the ordered list of IceSnow types (the grouping variable)\n",
    "# group is the ordered list of rows in each group\n",
    "for name, group in groups:\n",
    "    ax.plot(group.DD, group.Melt_Season_SWE_Change_m, marker='o', linestyle='', label=name)\n",
    "\n",
    "plt.title('Measured melt as fn of modeled PDD')\n",
    "plt.ylabel('Melt (m SWE)')\n",
    "plt.xlabel('Positive Degree Days (degC)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9841164-0c44-4692-a2b2-a591e4130349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE DEGREE DAY FACTOR\n",
    "\n",
    "# Should be in units (mm w.e. / deg C), typically ksub s (snow) or ksub i (ice)\n",
    "\n",
    "# convert melt and PDD to numpy arrays\n",
    "melt=wbclean['Melt_Season_SWE_Change_m'].to_numpy()\n",
    "# get melt in terms of mm\n",
    "meltmm=melt*1000\n",
    "PDD=wbclean['DD'].to_numpy() \n",
    "\n",
    "fig, ax=plt.subplots(figsize=(10,5))\n",
    "\n",
    "# create new variable, ddf, in wb. This is the degree day factor for each site at each year\n",
    "wbclean['ddf']=(wbclean.Melt_Season_SWE_Change_m/1000)/(wbclean.DD)\n",
    "\n",
    "# group entries by ice or snow\n",
    "groups = wbclean.groupby('IceSnow')\n",
    "\n",
    "# loop through each group\n",
    "# name is the ordered list of IceSnow types (the grouping variable)\n",
    "# group is the ordered list of rows in each group\n",
    "for name, group in groups:\n",
    "    ax.plot(group.DD, group.ddf, marker='o', linestyle='', label=name)\n",
    "\n",
    "plt.title('Melt factor as a function of DD at site')\n",
    "plt.ylabel('Melt factor (ddf: mm/deg C')\n",
    "plt.xlabel('Positive Degree Days (degC)')\n",
    "plt.legend()\n",
    "\n",
    "# label points\n",
    "# font1={'family':'serif','color':'darkred','weight':'normal','size':6}\n",
    "ssite=wbclean['Site'].to_numpy()\n",
    "ddf=wbclean['ddf'].to_numpy()\n",
    "label=f'({ssite})'\n",
    "for x,y,l in zip(PDD,ddf,ssite):\n",
    "    plt.annotate(l,(x,y),textcoords='offset points',xytext=(0,10),ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29924bac-e058-4cf1-b943-2c890033da9a",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "\n",
    "Degree Day Factors look pretty reasonable. About -5 to -8 mm/degC for ice, more variable -4 to 0 for snow. For ice they look pretty good, except they seem to be a little systematically low at Kenn 2700 and Kenn 2300. Could be my temp relationship doesn't have a big enough lapse rate, but also possible it's because they're anomalously close to moraines or just a little dirty. I think either is possible but we don't have good data to prove either, so we'll just leave them. For ice and snow they are more mixed up, but those are all sites that can potentially receive midsummer snow, especially at Gates 7200, where it isn't necessarily recorded as summer accumulation. In those cases we would find an anomalously low DDF, and I think especially those two with values >-1 that's the case.\n",
    "\n",
    "So, we'll do a regression of all ice sites to get pdd/ice, and we'll exclude the snow/ice completely, and for snow only we'll use all but the two >-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656690aa-edb6-456a-ac66-ea695d8f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do regressions to get DDFs for ICE (excludes \"suspect\" values)\n",
    "\n",
    "# create melt in mm column in wb\n",
    "wbclean['meltmm']=wbclean.Melt_Season_SWE_Change_m * 1000\n",
    "\n",
    "# ice only, meltice as a function of DD \n",
    "meltice = np.asarray(wbclean.meltmm.loc[wbclean.IceSnow=='ice/ice'])\n",
    "pddice = np.asarray(wbclean.DD.loc[wbclean.IceSnow=='ice/ice'])\n",
    "\n",
    "model=sm.OLS(meltice.astype(float),pddice.astype(float)).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df257b-329f-4297-991d-22697cdb4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do regressions to get DDFs for SNOW (excludes 'suspect' values)\n",
    "\n",
    "# create melt in mm column in wb (already done above)\n",
    "# wb['meltmm']=wb.Melt_Season_SWE_Change_m * 1000\n",
    "\n",
    "# snow only, meltsnow as a function of DD\n",
    "meltsnow = np.asarray(wbclean.meltmm.loc[wbclean.IceSnow=='snow/snow'])\n",
    "pddsnow = np.asarray(wbclean.DD.loc[wbclean.IceSnow=='snow/snow'])\n",
    "\n",
    "model=sm.OLS(meltsnow.astype(float),pddsnow.astype(float)).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb1441-1f71-453f-93e3-a179fba752db",
   "metadata": {},
   "source": [
    "## DDFs (excluding suspect values)\n",
    "\n",
    "### NPS and UAF values\n",
    "#### **Snow (with all snow/snow sites excluding suspect msmnts)** \n",
    "DDF for snow is -3.31 mm/PDD with R2 of 0.97 and F-statistic of 102.5. p<<.05. n=4\n",
    "\n",
    "#### **Ice (with all ice/ice sites excluding suspect msmnts)** **\n",
    "DDF for ice is -6.51 mm/PDD with R2 of 0.98 and F-statistic of 1503. p<<.05. n=26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c7842-94af-471e-ba80-1dd1259c3f5b",
   "metadata": {},
   "source": [
    "next steps: plot DDF plots for presentation\n",
    "\n",
    "plot march-november temp histories at each site, compare with measurement interval\n",
    "calculate pre and post-measurement melt at each site\n",
    "plot pre and post along with actual and modeled (mid-summer) at a few sites\n",
    "calculate full-season melt at all sites based on binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a37cc0-036a-4971-89c5-828e0b442c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE DATA FROM THIS NOTEBOOK SO I CAN START A NEW ONE\n",
    "\n",
    "## SAVE GatesNPS (temps at Gates)\n",
    "\n",
    "# rename nps dataset (this is the full record of hourly temps at Gates long-term RAWS)\n",
    "# index is 'AKdate' and temps column is 'GatesNPS'\n",
    "GatesNPS = nps\n",
    "\n",
    "## save data to pickle\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "\n",
    "# set pickle path (data_dir is directory, nps.pickle is dataset)\n",
    "GatesNPS_path = os.path.join(data_dir,'GatesNPS.pickle')\n",
    "\n",
    "# save variable dataframes\n",
    "GatesNPS.to_pickle(GatesNPS_path)\n",
    "\n",
    "\n",
    "## SAVE ablation (ablation stake data)\n",
    "\n",
    "# rename nps dataset (this is the full record of hourly temps at Gates long-term RAWS)\n",
    "# index is 'DateTime' and there are many columns\n",
    "ablation = wb\n",
    "\n",
    "## save data to pickle\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "\n",
    "# set pickle path (data_dir is directory, homogenized_hobos.pickle is dataset)\n",
    "ablation_path = os.path.join(data_dir,'ablation.pickle')\n",
    "\n",
    "# save variable dataframes\n",
    "ablation.to_pickle(ablation_path)\n",
    "\n",
    "\n",
    "## also should SAVE cleantemps and debristemps and centers and dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60846e8d-d898-48cd-950e-270ca079a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE EXERCISE\n",
    "## USE DD MODELS WITHOUT STAKE DATA EXPLICITLY TO CALCULATE GLACIER-WIDE MELT ON ICE ONLY\n",
    "\n",
    "## LOAD BINNED RASTER DATA\n",
    "\n",
    "# fullbins, debrisbins, and cleanbins are are panda dataframes\n",
    "# data are numbers of 5m x 5m pixels in each of 456 bins that separate pixels into 10m elevation bands. \n",
    "# So area in each elevation band is \"heights\" * 25 m.\n",
    "\n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "fullbins = pd.read_pickle(os.path.join(data_dir, 'fullbins.pickle'))\n",
    "cleanbins = pd.read_pickle(os.path.join(data_dir, 'cleanbins.pickle'))\n",
    "debrisbins = pd.read_pickle(os.path.join(data_dir, 'debrisbins.pickle'))\n",
    "\n",
    "# zero out all negative temperatures\n",
    "cleantemps[cleantemps<0]=0\n",
    "debristemps[debristemps<0]=0\n",
    "# remove nans\n",
    "np.nan_to_num(cleantemps,copy=False)\n",
    "np.nan_to_num(debristemps,copy=False)\n",
    "\n",
    "# create 4 new matrices to describe hourly melt as a function of time (dates) and elevation (centers).\n",
    "# four matrices will calculate for 2x2 matrix of debris/clean and snow/ice\n",
    "# base data are cleantemps and debristemps, plus ddf's\n",
    "# remember these ddfs (ddfice24 and ddfsnow24) are for a full day. Must divide by 24 to get hourly melt\n",
    "ddfice24=-6.60\n",
    "ddfsnow24=-3.31\n",
    "# values below are hourly melt\n",
    "ddfice = ddfice24/24\n",
    "ddfsnow = ddfsnow24/24\n",
    "\n",
    "# The four matrices below are DDF-based melt rates, IN MM/HR, of all dates and elevations on glacier\n",
    "cleansnowmelt=cleantemps*ddfsnow\n",
    "cleanicemelt=cleantemps*ddfice\n",
    "debrissnowmelt=debristemps*ddfsnow  # note that the debris-based temps wouldn't strictly speaking change during snowcover, but doesn't matter much\n",
    "debrisicemelt=debristemps*ddfice  # only this one would change if I had cameron's data\n",
    "\n",
    "# First try, just use cleanicemelt and debrisicemelt and don't worry about snow. See what I get.\n",
    "\n",
    "# extract heights from binned raster data and transpose so they are in a single row\n",
    "# double brackets make it 2d arraywith 456 rows and 1 column\n",
    "cleanheights=cleanbins[['heights']].to_numpy()\n",
    "debrisheights=debrisbins[['heights']].to_numpy()\n",
    "# multiply by 25 meters and transpose them to make them row vectors, not columns, of binned glacier area in square meters\n",
    "cleanarea=(cleanheights*25).T\n",
    "debrisarea=(debrisheights*25).T\n",
    "print(debrisarea.shape)\n",
    "# multiply melt times heights (and divide by 1000 to convert mm to m) to get matrix of actual volumetric hourly melt (m^3) in each debris band\n",
    "cleanicemeltbinned=cleanicemelt*cleanarea/1000\n",
    "cleansnowmeltbinned=cleansnowmelt*cleanarea/1000\n",
    "debrisicemeltbinned=debrisicemelt*debrisarea/1000\n",
    "debrissnowmeltbinned=debrissnowmelt*debrisarea/1000\n",
    "\n",
    "# get sums across columns to get total hourly glacier melt (all elevations combined) for any given hour\n",
    "hrcleanice=cleanicemeltbinned.sum(axis=1)\n",
    "hrcleansnow=cleansnowmeltbinned.sum(axis=1)\n",
    "hrdebrisice=debrisicemeltbinned.sum(axis=1)\n",
    "hrdebrissnow=debrissnowmeltbinned.sum(axis=1)\n",
    "totalicemelt=np.sum(hrcleanice)+np.sum(hrdebrisice)\n",
    "\n",
    "# RIGHT NOW JUST CALCULATING AS IF ENTIRE GLACIER WERE ICE: cum melt = -8599.94 km3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41595c1b-4cee-4510-845c-ea9390559a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ALTERNATE WAY OF CALCULATING DEBRIS-ICE-MELT, USING CAMERON AND ROUNCE DATA\n",
    "\n",
    "##### CAREFUL!!! SUPER LONG RUN TIME FOR THIS CELL!!\n",
    "\n",
    "# If you use this, comment out the debrisicemelt... line above\n",
    "# Here is the form we need to use, instead of y=-6.60. y=6.4669*np.exp(-.08150151*x). \n",
    "\n",
    "# This cell uses a raster of DDFs generated in arcmap by multiplying the rounce debris thickness map times DDF\n",
    "# where DDF=a*e^(-cx)+d where a is 5.93712, c is -0.10651, and d is 1.06660, which you can code as y=5.93712*np.exp(-0.10651*x)+1.06660.  \n",
    "# NOTE THAT X IS units CM. So in raster calculator you have to muliply x (thickness) * 100\n",
    "\n",
    "# get thickness tif \n",
    "# set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# Define relative path to file (this is different: DDFactors pre-calculated on a per-pixel basis)\n",
    "dem_path_rounceDDF = os.path.join(data_dir,\n",
    "                            r\"2013_rouncethick_DDFtry10.tif\")\n",
    "# get elev tif\n",
    "# Define relative path to file (this is the debris-only dem)\n",
    "dem_path_debris = os.path.join(data_dir,\n",
    "                            \"2013_debris_clip_elev.tif\")\n",
    "\n",
    "## TURN INTO ARRAYS\n",
    "# Open tif images and save as numpy arrays (im_elev_ar and im_DDF_ar)\n",
    "im_DDF=Image.open(dem_path_rounceDDF)\n",
    "im_DDF_ar=np.array(im_DDF)\n",
    "im_elev=Image.open(dem_path_debris)\n",
    "im_elev_ar=np.array(im_elev)\n",
    "\n",
    "# turn missing values-3.402823e+38 into 10,000 in DEM (always too cold to melt)\n",
    "im_elev_ar[im_elev_ar<-100]=10000\n",
    "# turn missing values in DDF into zeros (no data, no melt possible)\n",
    "im_DDF_ar[im_DDF_ar<0]=0\n",
    "\n",
    "# resample at daily interval to save time (npsdaily)\n",
    "npsdaily=nps.resample(rule='24H', offset=0).mean()\n",
    "\n",
    "# then save new dates vector and temps vector\n",
    "gatestempsD = npsdaily['GatesNPS'].to_numpy()\n",
    "datesD = npsdaily.index.to_numpy(dtype='datetime64[ns]')\n",
    "\n",
    "# pre-create a vector for saving melt at each timestep\n",
    "debrismeltcum = np.zeros(shape=[len(datesD),1])\n",
    "\n",
    "for i in range(len(datesD)):\n",
    "    # uncomment line below if you wantt to track progress\n",
    "    # print(i)\n",
    "    # calculate temps for that hoursteps for each pixel in those dems, at each time step t (from dates, 50567)\n",
    "    # where gatestemp is 50567 temp record and z is elevation of pixel\n",
    "    tempT = ((gatestempsD[i] * 0.571476) + 1.775760) - (0.00486 * (im_elev_ar - 1070))\n",
    "    # remove all negative temps\n",
    "    tempT[tempT<0]=0\n",
    "    # then for each pixel in those dems, at same time steps, calculate melt (in m SWE per 25m pixel per day) (*25 / 1000)\n",
    "    meltT = tempT * im_DDF_ar * 0.025\n",
    "    # now you have meltT, for one hour timestep, a raster of melt in m SWE per 25m pixel. \n",
    "    # need to bin or save it if you want to plot it, but if you only want to calculate melt (for now that's good enough)\n",
    "    # you can just sum all the melt in meltT and you'll have the total debris-covered melt for that timestep\n",
    "    debrismeltcum[i,0]=np.nansum(meltT)\n",
    "\n",
    "## SAVE DEBRISMELTCUM SEPARATELY FROM CELL ABOVE, IF NECESSARY\n",
    "\n",
    "# Save numpy .txt file debriscummelt_daily.txt with debrismeltcum in it. debrismeltcum is vector, 2141x0, \n",
    "# same shape as gatestempsD. Daily values year-round of cumulative melt values \n",
    "# on all debris-covered ice (hrdebrisice_cameron)\n",
    "    # set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# Define relative path to file \n",
    "debriscummelt_path = os.path.join(data_dir,\n",
    "                            \"debriscummelt_daily.txt\")\n",
    "np.savetxt(debriscummelt_path,debrismeltcum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = r'~/MLo/KennGlac/data/Markovsky data/Markovsky code_data 2021_12_10/actual_final_MLo.py'\n",
    "if os.path.exists(file_path):\n",
    "    print(file_path)\n",
    "else:\n",
    "    print('no')\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1928a-0c60-4c72-a7e9-e9c75b72ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE DEBRISMELTCUM SEPARATELY FROM CELL ABOVE, IF NECESSARY\n",
    "\n",
    "# Save numpy .txt file debriscummelt_daily.txt with debrismeltcum in it. debrismeltcum is vector, 2141x0, \n",
    "# same shape as gatestempsD. Daily values year-round of cumulative melt values \n",
    "# on all debris-covered ice (hrdebrisice_cameron)\n",
    "    # set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# Define relative path to file \n",
    "debriscummelt_path = os.path.join(data_dir,\n",
    "                            \"debriscummelt_daily.txt\")\n",
    "np.savetxt(debriscummelt_path,debrismeltcum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1f945-f089-4192-bab4-2aae60b08cd7",
   "metadata": {},
   "source": [
    "### Kenn River discharges\n",
    "from 5/1-10/30, currently calculated river discharges (km3) are\n",
    "\n",
    "2017: 1327.53 1327534323.8\n",
    "\n",
    "2018: 1512.19 1512185325.8\n",
    "\n",
    "2019: 1727.30 1727304096.8\n",
    "\n",
    "2020: 1348.18  1348182027.7\n",
    "\n",
    "2021: 1188.24  1188237807.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798610d8-20b8-41cd-99b7-75ff7a165bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT MELT RECORD \n",
    "\n",
    "%matplotlib widget\n",
    "fig,ax=plt.subplots(figsize=(10,6))\n",
    "plt.plot(dates,hrdebrisice,color='black',linewidth=0.5)\n",
    "plt.plot(dates,hrcleanice,color='red',linewidth=0.1)\n",
    "plt.xlabel('Melt (m^3/hr)')\n",
    "plt.ylabel('Modeled melt on debris-covered (black) and clean (red) ice')\n",
    "plt.title('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ffd20-a71d-4c1a-a31f-5efb3e0ee65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREP DAILY CAMERON DATA FOR COMPARISON WITH HOURLY DATA \n",
    "# \n",
    "# # resample at daily interval to save time as a daily value (npsdaily)\n",
    "npsdaily=nps.resample(rule='24H', offset=0).mean()\n",
    "\n",
    "# then save new dates vector and temps vector\n",
    "gatestempsD = npsdaily['GatesNPS'].to_numpy()\n",
    "datesD = npsdaily.index.to_numpy(dtype='datetime64[ns]')\n",
    "\n",
    "# Load numpy .txt file debriscummelt_daily.txt\n",
    "    # set working directory\n",
    "data_dir = r'~/MLo/KennGlac/data'\n",
    "# Define relative path to file \n",
    "debriscummelt_path = os.path.join(data_dir,\n",
    "                            \"debriscummelt_daily.txt\")\n",
    "debrismeltcum=np.loadtxt(debriscummelt_path)\n",
    "\n",
    "# debrismeltcum is vector, 2141x0, same shape as gatestempsD. Daily values year-round of cumulative melt values on all debris-covered ice (hrdebrisice_cameron)\n",
    "\n",
    "# create pandas table \"rouncedata\" and add debrismeltcum as a column\n",
    "rouncedata={'debrisice_cameron':debrismeltcum[:]}  # or [:,0]  ??\n",
    "rouncetable=pd.DataFrame(rouncedata,index=datesD)\n",
    "rouncetable.index.name = 'DateTime'\n",
    "# make sure index is format datetime\n",
    "rouncetable.index = pd.to_datetime(rouncetable.index)\n",
    "\n",
    "# Select data from 5/1 to 10/31 each year (rouncemeltseason is pd table (1104x1) of daily cum melt values with index DateTime\n",
    "rouncemeltseason = rouncetable[(rouncetable.index.month > 4) & (rouncetable.index.month <11)]\n",
    "year=rouncemeltseason.index.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115af3a-7a61-427a-ac83-9e6785795cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLICE MELT RECORDS INTO \n",
    "\n",
    "# Roughly speaking, lowest elevation glacier temps get consistenly postivie in mid-late march and last until end October\n",
    "# River discharge gets ice-free around end-April and lasts until freeze-up in early November\n",
    "# So I will be using May 1 - October 31 as the target for these calculations. \n",
    "\n",
    "# Make a pandas table (melt) of the melt records to facilitate slicing and summarizing\n",
    "data={'hrcleanice':hrcleanice,'hrdebrisice':hrdebrisice,'hrcleansnow':hrcleansnow,'hrdebrissnow':hrdebrissnow}\n",
    "# IF I did the cameron debris-covered thing at hourly timestep (I haven't), then use the row below instead of the one above\n",
    "# data={'hrcleanice':hrcleanice,'hrdebrisice_constant':hrdebrisice,'hrcleansnow':hrcleansnow,'hrdebrissnow':hrdebrissnow,'hrdebrisice_cameron':debrismeltcum}\n",
    "\n",
    "print(debrismeltcum.shape)\n",
    "melt=pd.DataFrame(data,index=dates)\n",
    "melt.index.name = 'DateTime'\n",
    "# make sure index is format datetime\n",
    "melt.index = pd.to_datetime(melt.index)\n",
    "\n",
    "\n",
    "# Select hourly data from 5/1 to 10/31 each year (I already selecte daily debris cameron data for that time period)\n",
    "meltseason = melt[(melt.index.month > 4) & (melt.index.month <11)]\n",
    "year=meltseason.index.year.unique()\n",
    "\n",
    "# create empty pandas table for results\n",
    "totalyearlymelt=pd.DataFrame(\n",
    "    columns=['hrcleanice','hrdebrisice_constant','hrcleansnow','hrdebrissnow','hrdebris_ice_cameron'],\n",
    "    index=[year])\n",
    "\n",
    "# Sum data by years (i). This adds in the data from the Cameron calculation\n",
    "for i in year:\n",
    "    tempp=meltseason[(meltseason.index.year == i)].sum()\n",
    "    temppp=-rouncemeltseason[(rouncemeltseason.index.year == i)].sum()\n",
    "    tempp_concat=np.concatenate((tempp,temppp), axis=0)\n",
    "    # use .values to get numbers out of weird object\n",
    "    totalyearlymelt.loc[i,:]=tempp_concat       #.values  \n",
    "\n",
    "print(totalyearlymelt.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c2fe4-65de-44c6-b31a-bc77012d2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT ANNUAL RESULTS\n",
    "\n",
    "# annualQ is from KennRivQ.ipynb, pulled 2021_12_6. This includes only 2017-2021 (no data from 2016)\n",
    "annualQ = [1328462087.04, 1512917264.74, 1727659577.95, 1348366520.09, 1188672046.56]\n",
    "\n",
    "# make figure ,presenting melt and discharge as positive numbers\n",
    "fig,ax=plt.subplots(figsize=(10,6))\n",
    "plt.plot(year,-totalyearlymelt.hrcleanice,color='red',linewidth=1, label = 'Melt on clean ice')\n",
    "plt.plot(year,-totalyearlymelt.hrdebrisice_constant,color='black',linewidth=1, label = 'Constant DDF melt on debris-covered ice')\n",
    "plt.plot(year,-totalyearlymelt.hrdebris_ice_cameron,color='black',linewidth=1,linestyle='dotted', label = 'Variable DDF melt on debris-covered ice')\n",
    "plt.plot(year,(-totalyearlymelt.hrcleanice - totalyearlymelt.hrdebrisice_constant),color='green',linewidth=2, label = 'Total melt w constant DDF',marker='o')\n",
    "plt.plot(year,(-totalyearlymelt.hrcleanice - totalyearlymelt.hrdebris_ice_cameron),color='green',linewidth=2, linestyle='dotted', label = 'Total melt w variable DDF',marker='o')\n",
    "plt.plot(year[1:6],annualQ,color='blue',linewidth=2, label = 'River discharge',marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Melt/discharge (m^3/yr)')\n",
    "ax.legend(loc='upper left',fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# save as high quality pdf\n",
    "fig.savefig('temp.pdf',bbox_inches='tight')\n",
    "\n",
    "annualQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2d404-278f-425f-a194-1629fa15120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First examine typical \"melt season\" for low elevation (lowest bin) glacier--we'll call that our river discharge season\n",
    "fig,ax=plt.subplots(figsize=(10,6))\n",
    "plt.plot(dates,debristemps[:,0])\n",
    "plt.plot(dates,debristemps[:,-1],color='red')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafe50c-e91b-4964-aa05-8a501d4be564",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(year[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797783d-935c-4007-8308-afc3badcb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalyearlymelt.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8a3f1-62ac-434e-95a2-3735001e45c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
